F1 score
The F1 score is a measure of a model’s performance. It is a weighted average of the 
precision and recall of a model, with results tending to 1 being the best, and those 
tending to 0 being the worst. You would use it in 
classification tests where true negatives don’t matter much.

Matthews correlation coefficient
t takes into account true and false positives and negatives and is generally regarded
as a balanced measure which can be used even if the classes are of very different sizes.
The MCC is in essence a correlation coefficient between the observed and predicted binary 
classifications; it returns a value between −1 and +1. A coefficient of +1 represents a 
perfect prediction, 0 no better than random prediction and −1 indicates total disagreement 
between prediction and observation.
