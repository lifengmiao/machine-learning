Bagging: (average strong models)

parallel ensemble: each model is built independently

aim to decrease variance, not bias (solve overfitting)

suitable for high variance low bias models (complex models)

an example of a tree based method is random forest, which develop fully grown trees (note that RF modifies the grown procedure to reduce the correlation between trees)

Boosting: (sequential weak models)

sequential ensemble: try to add new models that do well where previous models lack

aim to decrease bias, not variance (solve underfitting)

suitable for low variance high bias models

an example of a tree based method is gradient boosting
